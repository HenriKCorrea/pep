\chapter{Theoretical Background}
\label{chap:background}

\section{Digital Twins}

The concept of DTs has evolved significantly since its introduction by Grieves over two decades ago as a "conceptual ideal for product lifecycle management" \cite{Grieves_2017}.
Initially focused on manufacturing and product optimization, DTs have expanded to encompass entire complex systems such as 
    ports, cities, and supply chains \cite{Klar_2024}.
This expansion reflects the growing recognition of DTs as pillars of Industry 4.0 and innovation backbones for future systems \cite{Jiang_2021}.

At their core, Digital Twins represent virtual replicas of physical assets or processes that are continuously updated with real-time data throughout their lifecycle.
The Digital Twin Consortium provides a cross-domain definition characterizing DTs through three foundational elements:
    (1) virtual representation enabling comprehensive situational awareness,
    (2) real-world entities and processes analyzed within models, and
    (3) constant bidirectional data exchange synchronizing virtual and real-world entities \cite{Budiardjo_2021}.
This bidirectional nature distinguishes DTs from 
    digital shadows (unidirectional data flow) and 
    digital models (no automatic information flow) \cite{Botín-Sanabria_2022}.

The maturation of DT technologies has revealed significant challenges in achieving interoperability between heterogeneous systems.
As Klar et al. \cite{Klar_2024} argue, while existing maturity frameworks often emphasize autonomous operations as the highest level,
    true maturity in complex systems requires 
    \emph{interoperability}—the capability of DTs to optimize beyond their physical boundaries by exchanging information with interconnected systems.
This perspective aligns with the System of Systems (SoS) paradigm, where 
    joint optimization across domains delivers value greater than the sum of individual system optimizations \cite{Dietz_2020}.

The six-level maturity model proposed by Klar et al. \cite{Klar_2024} provides a domain-independent framework for assessing DT sophistication:
\begin{enumerate}
    \item \textbf{Replication of Assets}: Digital mirroring of physical assets and their states
    \item \textbf{Connection}: Integration of models with static data and metadata
    \item \textbf{Synchronization}: Enrichment with real-time sensor data
    \item \textbf{Interaction}: Bidirectional data communication and remote control
    \item \textbf{Automation}: Autonomous operations optimization and self-maintenance
    \item \textbf{Interoperability}: Joint decision-making across systems through standardized interfaces
\end{enumerate}

This research specifically addresses Level 6 interoperability,
    focusing on semantic interoperability challenges that enable DTs to cooperate while 
    preserving domain-specific optimizations and stakeholder autonomy.

\subsection{Digital Twins Frameworks}

Several open-source frameworks have emerged to support DT development, each with different architectural approaches and interoperability capabilities.

\textbf{KTWIN}~\cite{Wermann_Wickboldt_2025}
    is an open-source Kubernetes-based serverless platform for building and operating digital twins.
Developed as a novel approach to address the limitations of existing vendor-specific DT platforms,
    KTWIN provides a cloud-native, vendor-agnostic solution that enables deployment across 
    any Kubernetes cluster—whether on-premises, at the edge, or in multi-cloud environments.

KTWIN extends DTDL to bridge the gap between 
    domain modeling and technical deployment specifications.
While DTDL traditionally focuses on entity definitions and relationships,
    KTWIN's enhanced specification allows domain experts to configure
    underlying infrastructure aspects such as CPU allocation, memory requirements, 
    auto-scaling policies, and container deployment settings—all within the same ontology-based definition.

KTWIN's modular architecture comprises control plane and application plane components.
The control plane manages twin definitions, relationships, and orchestration through Kubernetes APIs,
    while the application plane handles real-time event processing, data storage, and twin services execution.
This separation enables independent scaling and evolution of different system aspects while 
    maintaining semantic consistency across the twin graph.

While KTWIN represents a significant advancement in open-source DT platforms,
    current implementations still rely on predefined routing patterns and centralized orchestration.
This creates opportunities for enhanced interoperability mechanisms that could enable
    more dynamic, decentralized collaboration between autonomous DT systems—
    a gap that agent-mediated approaches could address.

\textbf{Eclipse Ditto} is a framework for building digital twins of physical devices,
    focusing on managing and synchronizing the state between physical assets and their digital representations.
Ditto provides a rich API for interacting with digital twins and supports various communication protocols,
    making it particularly suited for IoT scenarios where device management and state synchronization are critical.

Both frameworks represent different approaches to DT implementation,
    but face challenges in achieving semantic interoperability across domains—a gap 
    this research aims to address through agent-mediated communication protocols.

% Digital Twin Consortium
The \textbf{Digital Twin Consortium}~\cite{Budiardjo_2021}
    has emerged as a key organization driving standardization and best practices in DT development.
Their Digital Twin System Interoperability Framework outlines seven interoperability concepts:
    system-centric design,
    model-based approach,
    holistic information flow,
    state-based interactions,
    federated repositories,
    actionable information, and
    scalable mechanisms.
This framework provides valuable guidance but primarily targets manufacturing contexts,
    leaving gaps in cross-domain semantic interoperability.

\section{Digital Environment}

The concept of a \emph{Digital Environment}, as introduced by~\cite{Nativi_2021} in the Destination Earth (DestinE) context, represents
    a paradigm shift in how complex systems interact digitally.
A Digital Environment can be understood as an ecosystem where multiple digital twins coexist, interact, and collaborate 
    to address complex, cross-domain challenges that individual systems cannot solve independently.

In the DestinE implementation, the Digital Environment architecture enables the creation of multiple digital twins
    representing different aspects of the Earth system (climate, oceans, atmosphere, etc.) that can
    interoperate through standardized interfaces and shared data spaces.
This approach recognizes that no single organization or system can encompass the complexity of global challenges,
    requiring instead a federated approach where specialized DTs maintain their autonomy
    while contributing to collective intelligence.

The Digital Environment concept addresses several key requirements for large-scale DT ecosystems:
\begin{itemize}
    \item \textbf{Federated Architecture}: Systems maintain independence while participating in collective workflows
    \item \textbf{Semantic Interoperability}: Meaningful data exchange across different ontological frameworks
    \item \textbf{Data Sovereignty}: Control over data sharing and usage rights
    \item \textbf{Scalable Collaboration}: Mechanisms for systems to discover and interact with relevant partners
\end{itemize}

While DestinE focuses on Earth system modeling, its architectural principles are highly relevant to other domains requiring cross-DT collaboration.
% However, current implementations rely heavily on predefined standards and centralized coordination, which
%     may not scale to dynamic, multi-stakeholder environments where systems evolve independently.

\section{AI Agents}

Artificial Intelligence Agents are computational systems that perceive their environment through
    sensors and take actions via actuators to achieve specific goals.
Russell and Norvig define an agent as
    "anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators"~\cite{Russell_2020}.
In the context of digital ecosystems, AI agents can act as autonomous intermediaries that
    facilitate communication, negotiation, and coordination between heterogeneous systems.

AI agents exhibit several key characteristics that make them particularly suitable for addressing DT interoperability challenges:
\begin{itemize}
    \item \textbf{Autonomy}:
    Ability to operate without direct human intervention,
        making decisions based on their programmed objectives and environmental observations
    \item \textbf{Reactivity}:
    Capacity to perceive environmental changes through
        sensors and respond appropriately in a timely manner
    \item \textbf{Pro-activeness}:
    Goal-directed behavior that enables agents to initiate interactions and
        take initiative to achieve their objectives
    \item \textbf{Social Ability}:
    Capability to communicate and cooperate with other agents and systems
        through agent communication languages and protocols
\end{itemize}

These characteristics enable AI agents to function as
    mediators that can bridge semantic gaps between different DT systems,
    facilitating interoperability without requiring extensive refactoring of existing infrastructures.

\subsection{AI Agent Protocols}

Agent protocols define the communication patterns and interaction mechanisms that enable coordinated behavior in multi-agent systems.
Several protocols have emerged as standards for agent-mediated communication.

\textbf{Model Context Protocol (MCP)}~\cite{Anthropic_2024}
    is an open-source standard that enables AI applications to
    connect to external data sources, tools, and workflows through
    standardized interfaces.
MCP implements a JSON-RPC 2.0 based exchange protocol—providing a unified way to
    connect AI systems to heterogeneous external systems including 
    databases, APIs, file systems, and specialized services.

MCP follows a client-server architecture where
    an AI application (MCP host) establishes connections to one or more MCP servers through dedicated MCP clients.
Each client maintains a one-to-one connection with its corresponding server, enabling the AI application to
    access distributed capabilities while preserving system boundaries.
The protocol operates through two distinct layers:
    a data layer that defines message structure and semantics, and
    a transport layer that manages communication channels through either standard input/output streams (stdio) for local processes or HTTP for remote servers.

The protocol defines three core primitives that servers can expose to clients:
\begin{itemize}
    \item \textbf{Tools}: Executable functions for performing actions
    \item \textbf{Resources}: Data sources providing contextual information
    \item \textbf{Prompts}: Reusable templates for structuring AI interactions
\end{itemize}
Additionally, MCP enables servers to access client capabilities through
\begin{itemize}
    \item \textbf{Sampling}: Requesting language model completions
    \item \textbf{Elicitation}: Requesting user input
    \item \textbf{Logging}: Sending debug information
\end{itemize}
This bidirectional capability exchange, combined with real-time notifications for dynamic updates,
    makes MCP particularly relevant for DT interoperability scenarios where
    systems need to discover, understand, and dynamically respond to each other's evolving capabilities
    while maintaining semantic coherence across different domain models.

\textbf{Agent-to-Agent (A2A) Protocol}~\cite{Google_A2A_2025}
    is an open standard that enables seamless communication and collaboration between AI agents, providing a
    common language for agents built using diverse frameworks and by different vendors.
Unlike approaches that wrap agents as tools (limiting their negotiation capabilities),
    A2A allows agents to communicate in their native modalities,
    preserving their autonomy while enabling complex multi-turn interactions such as negotiation and clarification.

A2A follows a client-server architecture with three core actors:
    users (human operators or automated services),
    A2A clients (applications or agents acting on behalf of users), and
    A2A servers (AI agents exposing HTTP endpoints implementing the A2A protocol).
The protocol emphasizes \emph{opaque execution}, where
    agents collaborate effectively without exposing their internal logic, memory, or proprietary tools,
    preserving intellectual property while enhancing security.

The protocol defines several fundamental communication elements that enable sophisticated agent interactions:
\begin{itemize}
    \item \textbf{Agent Cards}: JSON metadata documents describing an agent's identity, capabilities, endpoint, skills, and authentication requirements
    \item \textbf{Tasks}: Stateful units of work with unique IDs and defined lifecycles, facilitating tracking of long-running operations
    \item \textbf{Messages}: Single turns of communication containing content and roles, supporting multi-turn conversations
    \item \textbf{Artifacts}: Tangible outputs generated during task execution, providing structured and retrievable results
\end{itemize}

Both MCP and A2A protocols provide building blocks for the agent-mediated interoperability approach proposed in this research,
    addressing the semantic gaps that hinder current DT integration efforts.

\subsection{AI Agent Frameworks}

Several frameworks facilitate the development and deployment of AI agents, providing tools for agent orchestration, communication, and task execution.

\textbf{LangChain}~\cite{LangChainOverview}
    is a framework designed to be the easiest way to start building agents and applications powered by large language models.
With standardized model interfaces, LangChain enables developers to
    avoid vendor lock-in while building sophisticated agent systems.
The framework provides pre-built agent architectures and model integrations that facilitate
    rapid prototyping and development of LLM-powered applications.

LangChain's tool system allows LLMs to dynamically use external functions and access unstructured data,
    making it particularly suitable for creating agents that can mediate between different DT systems with natural language understanding capabilities.

\textbf{Agent Development Kit (ADK)}~\cite{googleAgentDevelopmentKit}
    is a flexible and modular framework developed by Google for developing and deploying AI agents.
While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and
    built for compatibility with other frameworks.
ADK provides tools for creating, deploying, and orchestrating AI agent architectures that range from simple tasks to complex workflows.

These frameworks, combined with specialized agent protocols, provide the
    technical foundation for implementing the digital ecosystem architecture proposed in this research—enabling
    autonomous DTs to achieve semantic interoperability without sacrificing their domain-specific optimizations.
