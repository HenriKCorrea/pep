\chapter{Theoretical Background}
\label{chap:background}

\section{Digital Twins}

The concept of DTs has evolved significantly since its introduction by Grieves over two decades ago as a "conceptual ideal for product lifecycle management" \cite{Grieves_2017}.
Initially focused on manufacturing and product optimization, DTs have expanded to encompass entire complex systems such as 
    ports, cities, and supply chains \cite{Klar_2024}.
This expansion reflects the growing recognition of DTs as pillars of Industry 4.0 and innovation backbones for future systems \cite{Jiang_2021}.

At their core, Digital Twins represent virtual replicas of physical assets or processes that are continuously updated with real-time data throughout their lifecycle.
The Digital Twin Consortium provides a cross-domain definition characterizing DTs through three foundational elements:
    (1) virtual representation enabling comprehensive situational awareness,
    (2) real-world entities and processes analyzed within models, and
    (3) constant bidirectional data exchange synchronizing virtual and real-world entities \cite{Budiardjo_2021}.
This bidirectional nature distinguishes DTs from 
    digital shadows (unidirectional data flow) and 
    digital models (no automatic information flow) \cite{Botín-Sanabria_2022}.

The maturation of DT technologies has revealed significant challenges in achieving interoperability between heterogeneous systems.
As Klar et al. \cite{Klar_2024} argue, while existing maturity frameworks often emphasize autonomous operations as the highest level,
    true maturity in complex systems requires 
    \emph{interoperability}—the capability of DTs to optimize beyond their physical boundaries by exchanging information with interconnected systems.
This perspective aligns with the System of Systems (SoS) paradigm, where 
    joint optimization across domains delivers value greater than the sum of individual system optimizations \cite{Dietz_2020}.

The six-level maturity model proposed by Klar et al. \cite{Klar_2024} provides a domain-independent framework for assessing DT sophistication:
\begin{enumerate}
    \item \textbf{Replication of Assets}: Digital mirroring of physical assets and their states
    \item \textbf{Connection}: Integration of models with static data and metadata
    \item \textbf{Synchronization}: Enrichment with real-time sensor data
    \item \textbf{Interaction}: Bidirectional data communication and remote control
    \item \textbf{Automation}: Autonomous operations optimization and self-maintenance
    \item \textbf{Interoperability}: Joint decision-making across systems through standardized interfaces
\end{enumerate}

This research specifically addresses Level 6 interoperability,
    focusing on semantic interoperability challenges that enable DTs to cooperate while 
    preserving domain-specific optimizations and stakeholder autonomy.

\subsection{Digital Twins Frameworks}

Several open-source frameworks have emerged to support DT development, each with different
    architectural approaches and interoperability capabilities.

\textbf{KTWIN}~\cite{Wermann_Wickboldt_2025}
    is an open-source Kubernetes-based serverless platform for building and operating digital twins.
Developed as a novel approach to address the limitations of existing vendor-specific DT platforms,
    KTWIN provides a cloud-native, vendor-agnostic solution that enables deployment across 
    any Kubernetes cluster—whether on-premises, at the edge, or in multi-cloud environments.

KTWIN extends DTDL to bridge the gap between 
    domain modeling and technical deployment specifications.
While DTDL traditionally focuses on entity definitions and relationships,
    KTWIN's enhanced specification allows domain experts to configure
    underlying infrastructure aspects such as CPU allocation, memory requirements, 
    auto-scaling policies, and container deployment settings—all within the same ontology-based definition.

KTWIN's modular architecture comprises control plane and application plane components.
The control plane manages twin definitions, relationships, and orchestration through Kubernetes APIs,
    while the application plane handles real-time event processing, data storage, and twin services execution.
This separation enables independent scaling and evolution of different system aspects while 
    maintaining semantic consistency across the twin graph.

While KTWIN represents a significant advancement in open-source DT platforms,
    current implementations still rely on predefined routing patterns and centralized orchestration.
This creates opportunities for enhanced interoperability mechanisms that could enable
    more dynamic, decentralized collaboration between autonomous DT systems—
    a gap that agent-mediated approaches could address.

\textbf{Eclipse Ditto}~\cite{eclipseEclipseDittoOpen2025}
    is a framework for building digital twins of physical devices,
    focusing on managing and synchronizing the state between physical assets and their digital representations.
Ditto provides a rich API for interacting with digital twins and supports various communication protocols,
    making it particularly suited for IoT scenarios where device management and state synchronization are critical.

Eclipse Ditto implements the digital twin software pattern in IoT contexts,
    mirroring potentially millions of physical "Things" (sensors, smart heating systems, connected cars, etc.)
    with their virtual, cloud-based counterparts in the digital world.
This approach simplifies IoT solution development by enabling software developers to interact with
    physical devices through their digital twins using standard web service APIs,
    without requiring knowledge of specific device connection protocols or physical locations.

Ditto's domain model provides a flexible, agnostic approach to Thing data structure through two core elements:
    \textbf{Attributes} for managing static metadata as JSON objects that change infrequently, and
    \textbf{Features} for managing dynamic state data such as sensor readings or configuration parameters.

While Ditto excels at device-level digital twin management and provides robust IoT integration capabilities,
    it primarily focuses on individual Thing synchronization rather than cross-domain semantic interoperability.
The framework's strength in maintaining device state consistency complements but does not address
    the broader challenges of enabling collaboration between heterogeneous DT systems with different ontologies—
    gaps that agent-mediated approaches could bridge.

Both KTWIN and Ditto represent different approaches to DT implementation,
    but face challenges in achieving semantic interoperability across domains—a gap 
    this research aims to address through agent-mediated communication protocols.

The \textbf{Digital Twin Consortium}~\cite{Budiardjo_2021}
    has emerged as a key organization driving standardization and best practices in DT development.
Their Digital Twin System Interoperability Framework outlines seven interoperability concepts:
    system-centric design,
    model-based approach,
    holistic information flow,
    state-based interactions,
    federated repositories,
    actionable information, and
    scalable mechanisms.
This framework provides valuable guidance but primarily targets manufacturing contexts,
    leaving gaps in cross-domain semantic interoperability.

\section{Digital Ecosystems}

The concept of a \emph{Digital Ecosystem} (DE), as introduced by~\cite{Nativi_2021} in the Destination Earth (DestinE) context, represents
    a paradigm shift in how complex systems interact digitally.
A Digital Ecosystem can be understood as an ecosystem where multiple digital twins coexist, interact, and collaborate 
    to address complex, cross-domain challenges that individual systems cannot solve independently.

Unlike traditional system-of-systems approaches that rely on predefined standards and centralized coordination,
    Digital Ecosystems embrace an evolutionary paradigm inspired by natural ecosystems.
This paradigm focuses on a holistic view of diverse and autonomous entities (the "biotic" component)
    sharing a common environment (the "abiotic" component) where they interact, evolve, and develop
    new collaborative strategies while modifying their shared environment.

The Digital Ecosystem approach addresses several critical requirements for large-scale DT collaboration:
\begin{itemize}
    \item \textbf{Federated Architecture}: Systems maintain independence while participating in collective workflows
    \item \textbf{Semantic Interoperability}: Meaningful data exchange across different ontological frameworks
    \item \textbf{Data Sovereignty}: Control over data sharing and usage rights
    \item \textbf{Scalable Collaboration}: Mechanisms for systems to discover and interact with relevant partners
    \item \textbf{Emergent Behavior}: Value generation through unpredictable interactions between autonomous systems
\end{itemize}

A DE constitutive mechanism is achieved when ecosystem-wide common value provides choices for
    enterprise systems to fulfill their expected enterprise value in holistic interactions.
This requires careful balance between belonging (to the ecosystem) versus autonomy (from the ecosystem),
    with these compromises evolving dynamically as systems adapt to changing requirements.

\subsection{Digital Ecosystem Information Architecture}

Digital Ecosystems dealing with DTs of the Earth share a set of main digital entities and enabling services
    that can be grouped into three principal strands: policy/business entities, framework/software entities,
    and constituent infrastructure entities.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/de-high-level-arch.png}
    \caption{Main high-level entities (and their simplified relationships) shared by a DE from~\cite{Nativi_2021}}
    \label{fig:de_info_arch}
\end{figure}

The information architecture illustrated in Figure~\ref{fig:de_info_arch} supports several conceptual and digital entities:

\textbf{Digital Twins (DTs)} represent the domain-specific virtual replicas recognized by use cases
    that the ecosystem supports, such as disaster risk management, climate adaptation, and digital oceans.
Each DT maintains bidirectional data exchange with its physical counterpart while
    participating in ecosystem-wide collaboration patterns.

\textbf{Digital Threads} constitute the flow of data fueling digital insights across the ecosystem,
    maintaining records of entities lifecycle from design to decommission.
Digital threads are crucial for conceiving, implementing, reusing, and assessing DTs,
    typically implemented through workflows and service orchestration processes that enable
    cross-system data lineage and provenance tracking.

\textbf{Digital/Virtual Resources} encompass distributed assets including:
    datasets with metadata schemas and associated data streams,
    process-based analytical software with scientific algorithms and workflows,
    learning-based AI analytical software with neural network models,
    and composite DTs that result from orchestrated combinations of elementary DT instances.

\textbf{Virtual Network Services} provide cloud-based functions that simplify ecosystem networking
    by bringing together distant and disparate resources efficiently.
These services implement multiple combinations of network functions across remote and cloud locations,
    optimize automation and orchestration services, and enable rapid service scaling.

This information architecture enables semantic interoperability across heterogeneous systems
    while preserving the autonomy and domain-specific optimizations of individual DTs.
The shared conceptual framework facilitates discovery, understanding, and dynamic collaboration
    without requiring extensive standardization of underlying system implementations.

% \subsection{Digital Ecosystem Virtual Cloud Layer and Orchestration Services}

% The Digital Ecosystem virtual cloud layer provides the technological foundation for
%     multi-cloud orchestration and resource management across distributed, heterogeneous infrastructures.
% This layer abstracts underlying complexity while enabling transparent access to
%     computing, storage, and analytical resources distributed across the ecosystem.

% \textbf{Virtual Cloud Paradigm}
% The virtual cloud implements a middleware framework that allows execution of tasks and applications
%     on multi-cloud environments, exposing them as unified, consistent virtual capacity.
% This approach enables organizations to increase efficiency by choosing appropriate services and providers
%     for different use cases while managing security and performance challenges in multi-cloud architectures.

% The virtual cloud layer virtualizes operational infrastructures through mediation and brokering services,
%     publishing open, common, and consistent network services and APIs to clients.
% This virtualization allows multi-cloud infrastructures to remain substantially autonomous and evolve freely,
%     avoiding propagation of changes to DTs and client applications while supporting transparent addition
%     of new operational infrastructures to enrich the ecosystem.

% \textbf{Orchestration Architecture}
% Virtual cloud orchestration utilizes software instruments to manage interconnections and interactions
%     among disparate systems constituting the multi-cloud infrastructure.
% The orchestration strategy connects automated tasks into cohesive workflows,
%     balancing multiple optimization criteria including data movement minimization,
%     analytics time lapse minimization, and energy consumption minimization
%     while preserving distributed data ownership and security constraints.

% The orchestration framework comprises several key components:
% \begin{itemize}
%     \item \textbf{Infrastructure Resources Orchestrator}: Implements resource orchestration and scalability
%         with interoperability arrangements for transparent job execution across different computing infrastructures
%     \item \textbf{Data and Analytics Software Broker}: Enables discoverability and access of distributed
%         data and analytical software resources, implementing required mediation and harmonization tasks
%     \item \textbf{Task Execution Optimizer}: Optimizes task execution based on configurable parameters,
%         recognizing optimal infrastructure placement and routing execution requests accordingly
% \end{itemize}

% This technological framework enables the separation of concerns across different stakeholder groups—
%     ecosystem users and clients, organizations steering the ecosystem,
%     and enterprises managing contributing infrastructures—while supporting both
%     Service-Oriented Architecture (SOA) and Resource-Oriented Architecture (ROA) patterns.

While Digital Ecosystems provide a robust framework for large-scale DT collaboration,
    current implementations still face challenges in achieving truly dynamic, autonomous interoperability.
The need for shared conceptual frameworks
    can create barriers to spontaneous collaboration between systems with different
    domain ontologies and operational constraints—gaps that agent-mediated approaches could address.

\section{AI Agents}

Artificial Intelligence Agents are computational systems that perceive their environment through
    sensors and take actions via actuators to achieve specific goals.
Russell and Norvig define an agent as
    "anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators"~\cite{Russell_2020}.
In the context of digital ecosystems, AI agents can act as autonomous intermediaries that
    facilitate communication, negotiation, and coordination between heterogeneous systems.

AI agents exhibit several key characteristics that make them particularly suitable for addressing DT interoperability challenges:
\begin{itemize}
    \item \textbf{Autonomy}:
    Ability to operate without direct human intervention,
        making decisions based on their programmed objectives and environmental observations
    \item \textbf{Reactivity}:
    Capacity to perceive environmental changes through
        sensors and respond appropriately in a timely manner
    \item \textbf{Pro-activeness}:
    Goal-directed behavior that enables agents to initiate interactions and
        take initiative to achieve their objectives
    \item \textbf{Social Ability}:
    Capability to communicate and cooperate with other agents and systems
        through agent communication languages and protocols
\end{itemize}

These characteristics enable AI agents to function as
    mediators that can bridge semantic gaps between different DT systems,
    facilitating interoperability without requiring extensive refactoring of existing infrastructures.

\subsection{AI Agent Protocols}

Agent protocols define the communication patterns and interaction mechanisms that enable coordinated behavior in multi-agent systems.
Several protocols have emerged as standards for agent-mediated communication.

\textbf{Model Context Protocol (MCP)}~\cite{Anthropic_2024}
    is an open-source standard that enables AI applications to
    connect to external data sources, tools, and workflows through
    standardized interfaces.
MCP implements a JSON-RPC 2.0 based exchange protocol—providing a unified way to
    connect AI systems to heterogeneous external systems including 
    databases, APIs, file systems, and specialized services.

MCP follows a client-server architecture where
    an AI application (MCP host) establishes connections to one or more MCP servers through dedicated MCP clients.
Each client maintains a one-to-one connection with its corresponding server, enabling the AI application to
    access distributed capabilities while preserving system boundaries.
The protocol operates through two distinct layers:
    a data layer that defines message structure and semantics, and
    a transport layer that manages communication channels through either standard input/output streams (stdio) for local processes or HTTP for remote servers.

The protocol defines three core primitives that servers can expose to clients:
\begin{itemize}
    \item \textbf{Tools}: Executable functions for performing actions
    \item \textbf{Resources}: Data sources providing contextual information
    \item \textbf{Prompts}: Reusable templates for structuring AI interactions
\end{itemize}
Additionally, MCP enables servers to access client capabilities through
\begin{itemize}
    \item \textbf{Sampling}: Requesting language model completions
    \item \textbf{Elicitation}: Requesting user input
    \item \textbf{Logging}: Sending debug information
\end{itemize}
This bidirectional capability exchange, combined with real-time notifications for dynamic updates,
    makes MCP particularly relevant for DT interoperability scenarios where
    systems need to discover, understand, and dynamically respond to each other's evolving capabilities
    while maintaining semantic coherence across different domain models.

\textbf{Agent-to-Agent (A2A) Protocol}~\cite{Google_A2A_2025}
    is an open standard that enables seamless communication and collaboration between AI agents, providing a
    common language for agents built using diverse frameworks and by different vendors.
Unlike approaches that wrap agents as tools (limiting their negotiation capabilities),
    A2A allows agents to communicate in their native modalities,
    preserving their autonomy while enabling complex multi-turn interactions such as negotiation and clarification.

A2A follows a client-server architecture with three core actors:
    users (human operators or automated services),
    A2A clients (applications or agents acting on behalf of users), and
    A2A servers (AI agents exposing HTTP endpoints implementing the A2A protocol).
The protocol emphasizes \emph{opaque execution}, where
    agents collaborate effectively without exposing their internal logic, memory, or proprietary tools,
    preserving intellectual property while enhancing security.

The protocol defines several fundamental communication elements that enable sophisticated agent interactions:
\begin{itemize}
    \item \textbf{Agent Cards}: JSON metadata documents describing an agent's identity, capabilities, endpoint, skills, and authentication requirements
    \item \textbf{Tasks}: Stateful units of work with unique IDs and defined lifecycles, facilitating tracking of long-running operations
    \item \textbf{Messages}: Single turns of communication containing content and roles, supporting multi-turn conversations
    \item \textbf{Artifacts}: Tangible outputs generated during task execution, providing structured and retrievable results
\end{itemize}

Both MCP and A2A protocols provide building blocks for the agent-mediated interoperability approach proposed in this research,
    addressing the semantic gaps that hinder current DT integration efforts.

\subsection{AI Agent Frameworks}

Several frameworks facilitate the development and deployment of AI agents, providing tools for agent orchestration, communication, and task execution.

\textbf{LangChain}~\cite{LangChainOverview}
    is a framework for building agents and LLM-powered applications.
Through standard interface for models, embeddings and vector data store,
    developers are capable to chain together interoperable components and third-party integrations,
    simplifying AI application development. 

LangChain's tool system allows LLMs to dynamically use external functions and access unstructured data,
    making it particularly suitable for creating agents that can mediate between different DT systems with natural language understanding capabilities.

\textbf{Agent Development Kit (ADK)}~\cite{googleAgentDevelopmentKit}
    is a flexible and modular framework developed by Google for developing and deploying AI agents.
While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and
    built for compatibility with other frameworks.
ADK provides tools for creating, deploying, and orchestrating AI agent architectures that range from simple tasks to complex workflows.

These frameworks, combined with specialized agent protocols, provide the
    technical foundation for implementing the digital ecosystem architecture proposed in this research—enabling
    autonomous DTs to achieve semantic interoperability without sacrificing their domain-specific optimizations.
