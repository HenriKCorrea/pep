% This chapter is the introduction and motivation for this research study plan.
% It should occupy two to three pages, providing an overview of the dissertation topic and the motivation for the work. 

\chapter{Introduction}
\label{chap:introduction}


Digital Twins (DTs) are virtual representations of physical entities that enable monitoring, simulation, and optimization of complex systems throughout their lifecycle~\cite{Botín-Sanabria_2022}.
By integrating data from sensors, IoT devices, and computational models, DTs provide actionable insights into the inner operations of systems and predict future behaviors in ways that traditional monitoring technologies cannot~\cite{Botín-Sanabria_2022}.
Recent advances in artificial intelligence (AI), machine learning (ML), big data analytics, and ubiquitous connectivity have accelerated DT adoption across diverse domains, including smart cities, energy grids, healthcare, manufacturing, and agriculture~\cite{Kshetri_2021}.


DT applications range from single-asset monitoring---such as Shell's structural DT for the Bonga oilfield's facility---to high-fidelity geographical representations like the European Commission's Destination Earth initiative~\cite{Kshetri_2021}.
These implementations demonstrate DTs' capacity to reduce operational costs, improve maintenance efficiency, and support decision-making under uncertainty.
However, as DT ecosystems expand, cross-domain use cases emerge that require cooperation between heterogeneous DT platforms.
For instance, a supply chain DT \cite{Gerlach_Zarnitz_2021} requires historical and real-time data from manufacturing DTs, warehousing DTs, and cargo DTs to optimize end-to-end operations.
As DT adoption grows, the need for interoperability between such heterogeneous platforms becomes increasingly critical to unlock cross-domain value and enable collaborative solutions that transcend the boundaries of individual systems.


Despite the growing adoption of DTs, achieving interoperability between heterogeneous DT systems remains a significant challenge~\cite{Tripathi_2024}.
Interoperability---the ability of different DT systems to seamlessly integrate and communicate despite using different standards, protocols, or ontologies---is critical for enabling cross-domain collaboration.
However, current integration approaches face several barriers.
First, \emph{technical interoperability} challenges arise from the diversity of proprietary platforms, data formats, and communication protocols used across domains.
Most industrial software applications are proprietary, preventing widespread adoption of open-source solutions and complicating system integration~\cite{Tripathi_2024}.
Second, \emph{semantic interoperability} remains difficult to achieve, as different domains model their systems using domain-specific ontologies (e.g., DTDL for smart cities, CIM for energy grids) that require manual mapping and translation.
Third, \emph{orchestration complexity} increases as multiple DTs must coordinate physical-to-physical, virtual-to-virtual, and virtual-to-physical interactions to solve complex cross-domain problems~\cite{Tripathi_2024}.
Finally, existing integration solutions often rely on static, pre-coordinated schemas that break when systems evolve, placing significant cognitive load on domain experts to maintain integrations over time.
These challenges highlight the need for flexible, non-intrusive interoperability solutions that can adapt to the diversity of existing DT implementations without requiring stakeholders to make disruptive changes to operational systems or abandon domain-optimized tools.


Recent advances propose the concept of \emph{Digital Ecosystems}---collaborative environments where autonomous DTs, stakeholders, and resources interact through open, common network services and APIs. Notably, Nativi et al.~\cite{nativi2022digital} present the Destination Earth Digital Ecosystem, which demonstrates how decoupling DTs from the underlying cloud infrastructure fosters both autonomy and cooperation among entities.


Building on this vision, this research explores the use of AI Agent Protocols, specifically the Model Context Protocol (MCP), to facilitate semantic interoperability between DTs. By leveraging agent-mediated negotiation and mapping, it becomes possible to bridge the gap between diverse ontologies and data models, accelerating DT maturity as described by Klar~\cite{klar2023digital}.


This work focuses on a cross-domain scenario involving Smart City and Smart Grid DTs, using the open-source KTWIN platform and integrating with an energy grid DT modeled in Eclipse Ditto. The goal is to demonstrate how agent protocols can enable meaningful cooperation between systems without requiring breaking changes or universal standards, thus advancing the state of DT interoperability.



% \section{Context and Motivation}


% Digital Twins (DTs) are virtual counterparts of physical systems that support monitoring, simulation, control, and decision support across domains such as manufacturing, mobility, healthcare, and smart cities \cite{Zhang_Wang_Cai_Wang_Guo_Zheng_2022}. The field has matured around heterogeneous technologies, ontologies, and runtime environments—this diversity is not a limitation but a fundamental strength, enabling domain-specific optimizations and the reuse of twin concepts across fundamentally different problem spaces. A smart city Digital Twin optimized for urban planning has radically different requirements than an avionics Digital Twin designed for UAV fleet management, and forcing convergence would sacrifice the specialized capabilities that make each valuable.
% % TODO: Add citations for LLM improvements as well.
% Yet, academic and industrial reports persistently frame this heterogeneity as a fundamental barrier. The increasing number of incompatible tools, data formats, and communication protocols prevents effective integration and simultaneous use of multiple DT systems for achieving common objectives. They advocate for universal Digital Twin development platforms that would standardize formats, protocols, and design methodologies \cite{Hu_Zhang_Deng_Liu_Tan_2021,Qi_Tao_Hu_Anwer_Liu_Wei_Wang_Nee_2021}. We propose to reframe the nature of the challenge: the issue is not that DT tools use different standards, but that current integration mechanisms lack the intelligence to bridge these differences dynamically when cross-domain collaboration is required. A recent interview study with 19 DT practitioners reports that multi-tool integration is the dominant approach precisely because cross-domain collaboration requires specialized capabilities \cite{Muctadir_2024}. The persistent challenges—interface incompatibilities and model-format mismatches arise from the static, brittle nature of current integration approaches.

% Current industry integration strategies fall into two categories: (a) accepting multi-tool complexity with static protocol bridges (e.g., OPC UA, FMI) that require extensive pre-coordination, or (b) collapsing to single-tool pipelines that sacrifice cross-domain agility \cite{Muctadir_2024}. Both approaches impose cognitive overhead on domain experts who must manually handle schema translation, orchestration logic, and system evolution management.

% Prior work shows that cloud-native platforms can significantly reduce infrastructure and vendor lock-in barriers. KTWIN, a serverless, Kubernetes-based DT platform, demonstrated this by abstracting deployment and operations with Knative and extending DTDL for both domain and system configuration \cite{Wermann_Wickboldt_2025}. However, after the compute and deployment layers are standardized, the harder problem remains: semantic and dynamic interoperability across heterogeneous DT systems. Today, integrations are depend on pre-defined schemas and contracts that do not adapt to evolving systems or unforeseen requirements.

% Recent advances in AI and agent protocols suggest a different path. The Model Context Protocol (MCP) provides a structured way for tools and clients to discover, browse, and act on contextual resources. In parallel, Agent-to-Agent (A2A) protocols enable autonomous agents to collaborate across vendor boundaries. Large Language Models (LLMs) can translate intents, map schemas, and synthesize glue code on demand. Together, protocol-anchored agents promise late-binding, intent-centric, and runtime-adaptive interoperability---not by unifying tools, but by mediating between them.
% % TODO: Add bib entries for MCP and A2A (e.g., \cite{mcp_spec,a2a_spec}) if available.

% \section{Problem Statement}

% Despite platform progress (e.g., KTWIN), DT interoperability remains predominantly:
% (1) syntactic, via static APIs and message formats;
% (2) pre-coordinated, requiring up-front schema alignment;
% (3) brittle, as integrations break when models or endpoints evolve; and
% (4) labor-intensive, placing cognitive load on domain experts to perform translation and orchestration.

% This research argues that LLM-driven agent protocols (e.g., MCP for context access and A2A for inter-agent negotiation) are a more suitable mechanism to achieve interoperability than attempting to enforce a unified DT tool. The thesis is that agents can provide:
% - semantic mediation (schema/ontology mapping across platforms),
% - intent mediation (goal-driven orchestration across tools),
% - dynamic mediation (runtime negotiation and adaptation as systems change).


