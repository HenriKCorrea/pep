% This chapter is the introduction and motivation for this research study plan.
% It should occupy two to three pages, providing an overview of the dissertation topic and the motivation for the work. 

\chapter{Introduction}

\section{Context and Motivation}


Digital Twins (DTs) are virtual counterparts of physical systems that support monitoring, simulation, control, and decision support across domains such as manufacturing, mobility, healthcare, and smart cities \cite{Zhang_Wang_Cai_Wang_Guo_Zheng_2022}. The field has matured around heterogeneous technologies, ontologies, and runtime environments—this diversity is not a limitation but a fundamental strength, enabling domain-specific optimizations and the reuse of twin concepts across fundamentally different problem spaces. A smart city Digital Twin optimized for urban planning has radically different requirements than an avionics Digital Twin designed for UAV fleet management, and forcing convergence would sacrifice the specialized capabilities that make each valuable.
% TODO: Add citations for LLM improvements as well.
Yet, academic and industrial reports persistently frame this heterogeneity as a fundamental barrier. The increasing number of incompatible tools, data formats, and communication protocols prevents effective integration and simultaneous use of multiple DT systems for achieving common objectives. They advocate for universal Digital Twin development platforms that would standardize formats, protocols, and design methodologies \cite{Hu_Zhang_Deng_Liu_Tan_2021,Qi_Tao_Hu_Anwer_Liu_Wei_Wang_Nee_2021}. We propose to reframe the nature of the challenge: the issue is not that DT tools use different standards, but that current integration mechanisms lack the intelligence to bridge these differences dynamically when cross-domain collaboration is required. A recent interview study with 19 DT practitioners reports that multi-tool integration is the dominant approach precisely because cross-domain collaboration requires specialized capabilities \cite{Muctadir_2024}. The persistent challenges—interface incompatibilities and model-format mismatches arise from the static, brittle nature of current integration approaches.

Current industry integration strategies fall into two categories: (a) accepting multi-tool complexity with static protocol bridges (e.g., OPC UA, FMI) that require extensive pre-coordination, or (b) collapsing to single-tool pipelines that sacrifice cross-domain agility \cite{Muctadir_2024}. Both approaches impose cognitive overhead on domain experts who must manually handle schema translation, orchestration logic, and system evolution management.

Prior work shows that cloud-native platforms can significantly reduce infrastructure and vendor lock-in barriers. KTWIN, a serverless, Kubernetes-based DT platform, demonstrated this by abstracting deployment and operations with Knative and extending DTDL for both domain and system configuration \cite{Wermann_Wickboldt_2025}. However, after the compute and deployment layers are standardized, the harder problem remains: semantic and dynamic interoperability across heterogeneous DT systems. Today, integrations are depend on pre-defined schemas and contracts that do not adapt to evolving systems or unforeseen requirements.

Recent advances in AI and agent protocols suggest a different path. The Model Context Protocol (MCP) provides a structured way for tools and clients to discover, browse, and act on contextual resources. In parallel, Agent-to-Agent (A2A) protocols enable autonomous agents to collaborate across vendor boundaries. Large Language Models (LLMs) can translate intents, map schemas, and synthesize glue code on demand. Together, protocol-anchored agents promise late-binding, intent-centric, and runtime-adaptive interoperability---not by unifying tools, but by mediating between them.
% TODO: Add bib entries for MCP and A2A (e.g., \cite{mcp_spec,a2a_spec}) if available.

\section{Problem Statement}

Despite platform progress (e.g., KTWIN), DT interoperability remains predominantly:
(1) syntactic, via static APIs and message formats;
(2) pre-coordinated, requiring up-front schema alignment;
(3) brittle, as integrations break when models or endpoints evolve; and
(4) labor-intensive, placing cognitive load on domain experts to perform translation and orchestration.

This research argues that LLM-driven agent protocols (e.g., MCP for context access and A2A for inter-agent negotiation) are a more suitable mechanism to achieve interoperability than attempting to enforce a unified DT tool. The thesis is that agents can provide:
- semantic mediation (schema/ontology mapping across platforms),
- intent mediation (goal-driven orchestration across tools),
- dynamic mediation (runtime negotiation and adaptation as systems change).


